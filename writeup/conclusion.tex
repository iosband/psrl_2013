\section{Conclusion}

We establish \emph{posterior sampling for reinforcement learning} not just as a heuristic, but as a provably efficient learning algorithm. We present regret bounds of  $\tilde{O}(\tau S \sqrt{AT} )$, which are some of the first for an algorithm not motivated by optimism and close to state of the art for any reinforcement learning algorithm. These bounds hold in expectation irrespective of prior, and with high probability unless the prior misspecification is exponentially large. PSRL is conceptually simple, computationally efficient and can easily incorporate prior knowledge; we demonstrate that PSRL performs well in simulation. Unusually for algorithms with guaranteed finite-time regret bounds, we have separated our \emph{algorithm} from our \emph{analysis}; as such, it may be possible for further analysis to provide even stronger guarantees on regret. We believe there is a strong case for the wider adoption of algorithms based upon posterior sampling in both theory and practice. 

Looking forward, we are interested in extending our analysis from the episodic case to infinite horizon learning without episodic reset, perhaps through exponentially growing episodes of posterior sampling. We also wonder whether it is possible to match the theoretical lower bounds for regret dependence on the states actions and time elapsed of $\sqrt{SAT}$ \cite{jaksch2010near} through a posterior sampling algorithm. Finally, since most problems of practical interest occur within extremely large, or even infinite, state and action spaces, we would like to explore variants of PSRL which can exploit some simplifying problem structure in these large spaces, such as linearity or factored MDPs.
