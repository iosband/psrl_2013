\section{Introduction}
We consider the classical reinforcement learning problem of an agent interacting with its environment while trying to maximize total reward accumulated over time \cite{burnetas1997optimal, kumar1986stochastic}. The agent's environment is 
modeled as a Markov decision process (MDP), but the agent is uncertain about the true dynamics of the MDP. As the agent interacts with its environment, it observes the outcomes that result from previous states and actions, and learns about the system dynamics. 
This leads to a fundamental tradeoff: by exploring poorly-understood states and actions the agent can learn to improve future performance, but it may attain better short-run performance by exploiting its existing knowledge.

Na\"ive optimization using point estimates for unknown variables overstates an agent's knowledge, which can lead to premature and suboptimal exploitation. To offset this, the majority of provably efficient learning algorithms use a principle known as \emph{optimism in the face of uncertainty} \cite{lai1985asymptotically} to encourage exploration. In such an algorithm, each state and actions is afforded some optimism bonus such that their value to the agent is modeled to be as high as statistically plausible. The agent will then choose a policy that is optimal under this ``optimistic" model of the environment. This incentivizes exploration since poorly-understood states and actions will receive a higher optimism bonus. As the agent resolves its uncertainty, the effect of optimism is reduced and the agent's behavior approaches optimality. Many authors have provided strong theoretical guarantees for optimistic algorithms \cite{jaksch2010near,bartlett2009regal,brafman2003r,kakade2003sample,kearns2002near}. In fact, almost all reinforcement learning algorithms with polynomial bounds on sample complexity employ optimism to guide exploration.

We study an alternative approach to efficient exploration, \emph{posterior sampling}, and provide finite time bounds on regret.  We model the agent's initial uncertainty over the environment through a prior distribution.\footnote{For an MDP, this might explicitly take the form of a prior over transition dynamics and reward distributions.}
At the start of each \emph{episode}, the agent chooses a new policy, which it follows for the duration of the episode. Posterior sampling for reinforcement learning (PSRL) selects this policy through two simple steps. First, a single instance of the environment is sampled from the posterior distribution at the start of an episode. Then, PSRL solves for and executes the policy that is optimal under the sampled environment over the episode. PSRL randomly selects policies according to the probability that they are optimal; exploration is guided by the variance of sampled policies as opposed to by optimism.

The idea of posterior sampling goes back to 1933 \cite{thompson1933} and has been applied successfully to multi-armed bandits. In that literature, the algorithm is often referred to as {\it Thompson sampling} or as {\it probability matching}. Despite its long history, posterior sampling was largely neglected by the multi-armed bandit literature until empirical studies \cite{chapelle2011empirical, scott2010modern} demonstrated that the algorithm could produce state of the art performance. This prompted a surge of interest, and a variety of strong theoretical guarantees are now available \cite{agrawal2012further, agrawal2013linear, kaufmann2012thompson, russo2013}. Our results suggest that this method has great potential in reinforcement learning as well.

PSRL was originally introduced in the context of reinforcement learning by \cite{strens2000bayesian} under the name ``Bayesian Dynamic Programming'',\footnote{We alter terminology to avoid any suggestion that PSRL is a Bayes-optimal solution, or approximates one directly.}
where it appeared primarily as a heuristic method.  In reference to PSRL and other ``Bayesian RL'' algorithms, \cite{kolter2009near} states ``little is known about these algorithms from a theoretical perspective, and it is unclear, what (if any) formal guarantees can be made for such approaches.''
Those Bayesian algorithms for which performance guarantees exist are guided by optimism. BOSS \cite{wang2005bayesian} introduces a more complicated version of PSRL that samples many MDPs, instead of just one, and then combines them into an \emph{optimistic} environment to guide exploration. BEB \cite{kolter2009near} adds an exploration bonus to states and actions according to how infrequently they have been visited. We show here that it is not always necessary to introduce optimism via a complicated construction, and, that the simple algorithm originally proposed in \cite{strens2000bayesian} satisfies strong bounds itself.

Our work is motivated by several advantages of posterior sampling relative to optimistic algorithms. First, since PSRL only requires solving for an optimal policy for a single sampled MDP, it is computationally cheap. Many optimistic methods require a simultaneous optimization across a \emph{family} of plausible environments \cite{jaksch2010near,bartlett2009regal,wang2005bayesian} while those that attempt to approximate the Bayes-optimal solution directly are generally computationally intensive \cite{wang2005bayesian,guez2012efficient, asmuth2011approaching}. Second, the presence of an explicit prior allows an agent to incorporate known environment structure in a natural way. This is crucial for any practical application, where learning without any prior knowledge would be intractable. Finally, posterior sampling allows us to separate the \emph{algorithm} from the \emph{analysis}. In any optimistic algorithm, performance is greatly influenced by the manner in which optimism is implemented. Past works have designed algorithms, at least in part, to facilitate theoretical analysis. Although our analysis of posterior sampling is closely related to the analysis in \cite{jaksch2010near}, this worst-case bound has no impact on the algorithm's actual performance. We show in section 6 that PSRL outperforms the optimistic algorithm UCRL2 \cite{jaksch2010near}, a competitor with similar regret bounds.
