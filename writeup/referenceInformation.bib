@article{jaksch2010near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  journal={The Journal of Machine Learning Research},
  volume={99},
  pages={1563--1600},
  year={2010},
  publisher={MIT Press}
}

@book{lehmann1998theory,
  title={Theory of point estimation},
  author={Lehmann, Erich Leo and Casella, George},
  volume={31},
  year={1998},
  publisher={Springer}
}

@inproceedings{abbasi2009forced,
  title={Forced-exploration based algorithms for playing in stochastic linear bandits},
  author={Abbasi-Yadkori, Y. and Antos, A. and Szepesv{\'a}ri, C.},
  booktitle={COLT Workshop on On-line Learning with Limited Feedback},
  year={2009}
}

@inbook{gittins2011multi,
author = {Gittins, John and Glazebrook, Kevin and Weber, Richard},
publisher = {John Wiley \& Sons, Ltd},
isbn = {9780470980033},
title = {Multi-Armed Bandit Allocation Indices},
year = {2011}
}

@article{bubeck2012regret,
  title={Regret analysis of stochastic and nonstochastic multi-armed bandit problems},
  author={Bubeck, S. and Cesa-Bianchi, N.},
  journal={arXiv preprint arXiv:1204.5721},
  year={2012}
}

@article{ryzhov2012knowledge,
  title={The knowledge gradient algorithm for a general class of online learning problems},
  author={Ryzhov, I.O. and Powell, W.B. and Frazier, P.I.},
  journal={Operations Research},
  volume={60},
  number={1},
  pages={180--195},
  year={2012},
  publisher={INFORMS}
}

@inproceedings{aminbandits,
  title={Bandits, Query Learning, and the Haystack Dimension},
  author={Amin, K. and Kearns, M. and Syed, U.},
  booktitle={Proceedings of the 24th Annual Conference on Learning Theory (COLT)},
  year={2011}
}

@inproceedings{dudik2011efficient,
  title={Contextual bandit algorithms with supervised learning guarantees},
  author={Beygelzimer, A. and Langford, J. and Li, L. and Reyzin, L. and Schapire, R.E.},
  booktitle={Conference on Artificial Intelligence and Statistics (AISTATS)},
  publisher={JMLR Workshop and Conference Proceedings},
  volume={15},
  year={2011}
}

@article{agarwal2012contextual,
  title={Contextual Bandit Learning with Predictable Rewards},
  author={Agarwal, A. and Dud{\'i}k, M. and Kale, S. and Langford, J. and Schapire, R.E.},
  journal={arXiv preprint arXiv:1202.1334},
  year={2012}
}

@article{abbasi2011improved,
  title={Improved algorithms for linear stochastic bandits},
  author={Abbasi-Yadkori, Y. and P{\'a}l, D. and Szepesv{\'a}ri, C.},
  journal={Advances in Neural Information Processing Systems},
  volume={24},
  year={2011}
}

@inproceedings{abbasi2012online,
  title={Online-to-confidence-set conversions and application to sparse stochastic bandits},
  author={Abbasi-Yadkori, Y. and Pal, D. and Szepesv{\'a}ri, C.},
  booktitle={Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2012}
}

@inproceedings{agrawal2012analysis,
  title={Analysis of {T}hompson Sampling for the multi-armed bandit problem},
  author={Agrawal, S. and Goyal, N.},
  journal={Proceedings of the 21st Annual Conference on Learning Theory (COLT)},
  year={2012}
}

@inproceedings{dani2008stochastic,
  title={Stochastic linear optimization under bandit feedback},
  author={Dani, V. and Hayes, T.P. and Kakade, S.M.},
  booktitle={Proceedings of the 21st Annual Conference on Learning Theory (COLT)},
  pages={355--366},
  year={2008}
}

@inproceedings{chapelle2011empirical,
  title={An empirical evaluation of {T}hompson sampling},
  author={Chapelle, O. and Li, L.},
  booktitle={Neural Information Processing Systems (NIPS)},
  year={2011}
}

@inproceedings{kaufmann2012bayesian,
  title={On {B}ayesian Upper Confidence Bounds for Bandit Problems},
  author={Kaufmann, E. and Capp{\'e}, O. and Garivier, A.},
  booktitle={Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2012}
}

@article{may2012optimistic,
  title={Optimistic {B}ayesian sampling in contextual-bandit problems},
  author={May, B.C. and Korda, N. and Lee, A. and Leslie, D.S.},
  journal={The Journal of Machine Learning Research},
  volume={98888},
  pages={2069--2106},
  year={2012},
  publisher={JMLR.org}
}

@inproceedings{kaufmann2012thompson,
  author = {E. Kauffmann and N. Korda and R. Munos},
  title = {Thompson Sampling: an Asymptotically Optimal Finite Time Analysis},
  booktitle = {International Conference on Algorithmic Learning Theory},
  pdf = {./files/Thompson_ALT2012.pdf},
  x-editorial-board = {yes},
  x-proceedings = {yes},
  x-international-audience = {yes},
  abstract = {The question of the optimality of Thompson Sampling for solving the stochastic multi-armed bandit problem had been open since 1933. In this paper we answer it positively for the case of Bernoulli rewards by providing the first finite-time analysis that matches the asymptotic rate given in the Lai and Robbins lower bound for the cumulative regret. The proof is accompanied by a numerical comparison with other optimal policies, experiments that have been lacking in the literature until now for the Bernoulli case.},
  year = {2012}
}

@article{KL-UCB2012,
  author = {O. Capp{\'e} and A. Garivier and O.-A. Maillard and R. Munos and G. Stoltz},
  title = {Kullback-{L}eibler Upper Confidence Bounds for Optimal Sequential Allocation},
  journal = {Submitted to the Annals of Statistics},
  pdf = {./files/klucb2012.pdf},
  x-editorial-board = {yes},
  x-proceedings = {yes},
  x-international-audience = {yes},
  year = {2012}
}

@article{scott2010modern,
  title={A modern {B}ayesian look at the multi-armed bandit},
  author={Scott, S.L.},
  journal={Applied Stochastic Models in Business and Industry},
  volume={26},
  number={6},
  pages={639--658},
  year={2010},
  publisher={Wiley Online Library}
}

@article{agrawal2013linear,
  title={Thompson Sampling for Contextual Bandits with Linear Payoffs},
  author={Agrawal, S. and Goyal, N.},
  journal={arXiv preprint arXiv:1209.3352},
  year={2012}
}

@article{agrawal2012further,
  title={Further Optimal Regret Bounds for {T}hompson Sampling},
  author={Agrawal, S. and Goyal, N.},
  journal={arXiv preprint arXiv:1209.3353},
  year={2012}
}

@article{filippi2010parametric,
  title={Parametric bandits: The generalized linear case},
  author={Filippi, S. and Capp{\'e}, O. and Garivier, A. and Szepesv{\'a}ri, C.},
  journal={Advances in Neural Information Processing Systems},
  volume={23},
  pages={1--9},
  year={2010}
}

@article{auer2003using,
  title={Using confidence bounds for exploitation-exploration trade-offs},
  author={Auer, P.},
  journal={The Journal of Machine Learning Research},
  volume={3},
  pages={397--422},
  year={2003},
  publisher={JMLR. org}
}

@article{auer2002finite,
  title={Finite-time analysis of the multiarmed bandit problem},
  author={Auer, P. and Cesa-Bianchi, N. and Fischer, P.},
  journal={Machine learning},
  volume={47},
  number={2},
  pages={235--256},
  year={2002},
  publisher={Springer}
}

@article{auer2002nonstochastic,
  title={The nonstochastic multiarmed bandit problem},
  author={Auer, P. and Cesa-Bianchi, N. and Freund, Y. and Schapire, R.E.},
  journal={SIAM Journal on Computing},
  volume={32},
  number={1},
  pages={48--77},
  year={2002},
  publisher={SIAM}
}

@article{bubeck2011xarmed,
 title={X-armed bandits},
 author={Bubeck, S. and Munos, R. and Stoltz, G. and Szepesv{\'a}ri, C. },
 journal={Journal of Machine Learning Research},
 pages={1587â€“1627},
 volume={12},
 year={2011}
}

@inproceedings{liopen2012,
  title={Open Problem: Regret Bounds for {T}hompson Sampling},
  author={Li, L. and Chapelle, O.},
  booktitle={Proceedings of the 25th Annual Conference on Learning Theory (COLT)},
  year={2012}
}

@article{gittins1979dynamic,
  title={A dynamic allocation index for the discounted multiarmed bandit problem},
  author={Gittins, J.C. and Jones, D.M.},
  journal={Biometrika},
  volume={66},
  number={3},
  pages={561--565},
  year={1979},
  publisher={Biometrika Trust}
}

@inproceedings{kleinberg2008multi,
  title={Multi-armed bandits in metric spaces},
  author={Kleinberg, R. and Slivkins, A. and Upfal, E.},
  booktitle={Proceedings of the 40th ACM Symposium on Theory of Computing},
  year={2008}
}

@article{lai1985asymptotically,
  title={Asymptotically efficient adaptive allocation rules},
  author={Lai, T.L. and Robbins, H.},
  journal={Advances in applied mathematics},
  volume={6},
  number={1},
  pages={4--22},
  year={1985},
  publisher={Elsevier}
}

@article{mersereau2009structured,
  title={A structured multiarmed bandit problem and the greedy policy},
  author={Mersereau, A.J. and Rusmevichientong, P. and Tsitsiklis, J.N.},
  journal={Automatic Control, IEEE Transactions on},
  volume={54},
  number={12},
  pages={2787--2802},
  year={2009},
  publisher={IEEE}
}

@article{rusmevichientong2010linearly,
  title={Linearly parameterized bandits},
  author={Rusmevichientong, P. and Tsitsiklis, J.N.},
  journal={Mathematics of Operations Research},
  volume={35},
  number={2},
  pages={395--411},
  year={2010},
  publisher={INFORMS}
}

@article{srinivas2012information,
author={Srinivas, N. and Krause, A. and Kakade, S.M. and Seeger, M.}, 
journal={Information Theory, IEEE Transactions on}, 
title={Information-Theoretic Regret Bounds for {G}aussian Process Optimization in the Bandit Setting}, 
year={2012}, 
month={may }, 
volume={58}, 
number={5}, 
pages={3250 -3265}, 
keywords={Bayesian methods;Convergence;{G}aussian processes;Kernel;Noise;Optimization;Temperature sensors;{G}aussian processes;Hilbert spaces;information theory;{G}aussian process optimization;bandit setting;cumulative regret;information-theoretic regret bounds;intuitive {G}aussian process upper confidence bound algorithm;multiarmed bandit problem;payoff function;reproducing kernel Hilbert space;sublinear regret bounds;Bandit problems;Bayesian prediction;{G}aussian process (GP);experimental design;information gain;nonparametric statistics;online learning;regret bound;statistical learning;}, 
doi={10.1109/TIT.2011.2182033}, 
ISSN={0018-9448},}


@article{thompson1933,
author={W. R. Thompson},
 title={On the likelihood that one unknown probability exceeds another in view of the evidence of two samples},
 journal={Biometrika},
 year={1933},
 volume={25},
 number={3/4},
 pages={285-294}
}


@article{sahni1974,
 title={Computationally related problems},
 author={Sahni, A.},
 journal={SIAM Journal on Computing},
 year={1974},
 volume={3},
 number={4},
 pages={262-279}
}


@inproceedings{strens2000bayesian,
  title={A {B}ayesian framework for reinforcement learning},
  author={Strens, Malcolm},
  booktitle={Proceedings of the 17th International Conference on Machine Learning},
  pages={943--950},
  year={2000}
}

@book{kumar1986stochastic,
  title={Stochastic systems: estimation, identification and adaptive control},
  author={Kumar, Panqanamala Ramana and Varaiya, Pravin},
  year={1986},
  publisher={Prentice-Hall, Inc.}
}

@article{burnetas1997optimal,
  title={Optimal adaptive policies for Markov decision processes},
  author={Burnetas, Apostolos N and Katehakis, Michael N},
  journal={Mathematics of Operations Research},
  volume={22},
  number={1},
  pages={222--255},
  year={1997},
  publisher={INFORMS}
}

@article{kearns2002near,
  title={Near-optimal reinforcement learning in polynomial time},
  author={Kearns, Michael and Singh, Satinder},
  journal={Machine Learning},
  volume={49},
  number={2-3},
  pages={209--232},
  year={2002},
  publisher={Springer}
}

@article{brafman2003r,
  title={R-max-a general polynomial time algorithm for near-optimal reinforcement learning},
  author={Brafman, Ronen I and Tennenholtz, Moshe},
  journal={The Journal of Machine Learning Research},
  volume={3},
  pages={213--231},
  year={2003},
  publisher={JMLR. org}
}

@phdthesis{kakade2003sample,
  title={On the sample complexity of reinforcement learning},
  author={Kakade, Sham Machandranath},
  year={2003},
  school={University of London}
}

@inproceedings{bartlett2009regal,
  title={REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs},
  author={Bartlett, Peter L and Tewari, Ambuj},
  booktitle={Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
  pages={35--42},
  year={2009},
  organization={AUAI Press}
}

@article{martin1967bayesian,
  title={Bayesian decision problems and Markov chains},
  author={Martin, James John},
  year={1967},
  publisher={Wiley}
}

@inproceedings{wang2005bayesian,
  title={Bayesian sparse sampling for on-line reward optimization},
  author={Wang, Tao and Lizotte, Daniel and Bowling, Michael and Schuurmans, Dale},
  booktitle={Proceedings of the 22nd international conference on Machine learning},
  pages={956--963},
  year={2005},
  organization={ACM}
}

@article{mundhenk2000complexity,
  title={Complexity of finite-horizon Markov decision process problems},
  author={Mundhenk, Martin and Goldsmith, Judy and Lusena, Christopher and Allender, Eric},
  journal={Journal of the ACM (JACM)},
  volume={47},
  number={4},
  pages={681--720},
  year={2000},
  publisher={ACM}
}

@inproceedings{kolter2009near,
  title={Near-{B}ayesian exploration in polynomial time},
  author={Kolter, J Zico and Ng, Andrew Y},
  booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
  pages={513--520},
  year={2009},
  organization={ACM}
}

@inproceedings{dearden1999model,
  title={Model based {B}ayesian exploration},
  author={Dearden, Richard and Friedman, Nir and Andre, David},
  booktitle={Proceedings of the fifteenth Conference on Uncertainty in Artificial Intelligence},
  pages={150--159},
  year={1999},
  organization={Morgan Kaufmann Publishers Inc.}
  }
  
  @inproceedings{asmuth2009bayesian,
  title={A {B}ayesian sampling approach to exploration in reinforcement learning},
  author={Asmuth, John and Li, Lihong and Littman, Michael L and Nouri, Ali and Wingate, David},
  booktitle={Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
  pages={19--26},
  year={2009},
  organization={AUAI Press}
}

@article{russo2013,
  author    = {Daniel Russo and
               Benjamin Van Roy},
  title     = {Learning to Optimize Via Posterior Sampling},
  journal   = {CoRR},
  volume    = {abs/1301.2609},
  year      = {2013},
  ee        = {http://arxiv.org/abs/1301.2609},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@article{strehl2008analysis,
  title={An analysis of model-based interval estimation for Markov decision processes},
  author={Strehl, Alexander L and Littman, Michael L},
  journal={Journal of Computer and System Sciences},
  volume={74},
  number={8},
  pages={1309--1331},
  year={2008},
  publisher={Elsevier}
}

@article{guez2012efficient,
  title={Efficient bayes-adaptive reinforcement learning using sample-based search},
  author={Guez, Arthur and Silver, David and Dayan, Peter},
  journal={arXiv preprint arXiv:1205.3109},
  year={2012}
}

@inproceedings{asmuth2011approaching,
  title={Approaching Bayes-optimalilty using Monte-Carlo tree search},
  author={Asmuth, John and Littman, ML},
  booktitle={Proc. 21st Int. Conf. Automat. Plan. Sched., Freiburg, Germany},
  year={2011}
}

