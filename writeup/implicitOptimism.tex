\section{True versus sampled MDP}

A simple observation that is central to our analysis is that, at the start of each $k$th episode, $\opt$ and $\sampled$ are identically distributed.
This fact allows us to relate quantities that depend on the true, but unknown, MDP $\opt$, to those of the sampled MDP $\sampled$, which is fully observed by the agent. We introduce $\history$ as the $\sigma$-algebra generated by the history up to $t_k$. Readers unfamiliar with measure theory can think of this as ``all information known just before the start of period $t_k$". When we say that a random variable X is $\history$-measurable, this intuitively means that although X is random, it is deterministically known given the information contained in $H_{t_k}$.The following lemma is an immediate consequence of this observation \cite{russo2013}.

\begin{lemma}[Posterior Sampling]
\label{lem: fundemental}
%For $\sampled$ sampled from Bayesian posterior with true parameter $\opt$ then 
If $f$ is the distribution of $M^*$ then, for any $\history$-measurable function $g$,
\begin{equation}\label{eq: fundemental lemma}
\Exp [g(\opt) | H_{t_k}] = \Exp[g(\sampled) | H_{t_k}].
\end{equation}
\end{lemma}

Note that taking the expectation of \eqref{eq: fundemental lemma} shows that $\Exp [g(\opt)] = \Exp[g(\sampled)]$ through the tower property. 
Another key lemma that will be used several times in our analysis, is the Azuma-Hoeffding inequality, which gives a concentration guarantee for martingales with bounded differences.

\begin{lemma}[Azuma-Hoeffding]
\label{lem: Azuma}
If $Y_n$ is a zero-mean martingale with almost surely bounded increments $|Y_i -Y_{i-1}| \leq C$ , then for any $\delta >0$ with probability at least $1-\delta$, $Y_n \leq C\sqrt{2n \log(1/\delta)}$.
\end{lemma}

Recall, we have defined $\Delta_k = \sum_{s \in \states} \rho(s) (V^{M^*}_{\mu^*, 1}(s) - V^{M^{*}}_{\mu_k, 1}(s))$,  to be the regret over period $k$. A significant hurdle in analyzing this equation is its dependence on the optimal policy $\optPol$, which we do not observe. For many reinforcement learning algorithms, there is no clean way to relate the unknown optimal policy to the states and actions the agent actually observes. The following result shows we how we can avoid this issue using Lemma \ref{lem: fundemental}. First, define 
\begin{equation} \label{eq: deltaTilde}
\tilde{\Delta}_k = \sum_{s \in \states} \rho(s) (V^{M_k}_{\mu_k, 1}(s) - V^{M^{*}}_{\mu_k, 1}(s))
\end{equation}

as the difference in expected value of the policy $\mu_k$ under the sampled MDP $M_k$, which is known, and its performance under the true MDP $M^*$, which is observed through the agent's experience.

\begin{theorem}[Regret equivalence]
\begin{equation}
\Exp \left[ \sum_{k=1}^{m} \Delta_k \right] =\Exp \left[ \sum_{k=1}^{m} \tilde{\Delta}_k \right] 
\end{equation}
and for any $\delta>0$ with probability at least $1-\delta$, 
\begin{equation} \label{eq: from Delta to Delta tilde}
\sum_{k=1}^{m} \Delta_k \leq \sum_{k=1}^{m} \tilde{\Delta}_k +\tau \sqrt{2m \log(1/\delta)} 
\end{equation}
\end{theorem}
\begin{proof}
Note,  $\Delta_k - \tilde{\Delta}_k =\sum_{s \in \states} \rho(s) (V^{M^*}_{\mu^*, 1}(s) - V^{M_k}_{\mu_k, 1}(s)) \in [-\tau, \tau]$. By Lemma \ref{lem: fundemental}, $\Exp[\Delta_k - \tilde{\Delta}_k \vert H_{t_k}] =0 $. Taking expectations therefore establishes the first claim. The second claim follows by applying Lemma \ref{lem: Azuma} to $\sum_{k=1}^{m} \Delta_k - \tilde{\Delta}_k$, which is a zero mean martingale with respect to the filtration $\left\{ H_{t_k}: k=1,..,m \right\}$. 
\end{proof}

This result bounds the agent's regret in epsiode $k$ by the difference between the agent's estimate $V^{M_k}_{\mu_k, 1}(s_{t_k})$ of the expected reward in $M_k$ from the policy it chooses, and the expected reward $V^{M^*}_{\mu_k, 1}(s_{t_k})$ in $M^*$. If the agent has a poor estimate of the MDP $M^*$, we expect it to learn as the performance of following $\mu_k$ under $M^*$ differs from its expectation under $M_k$. As more information is gathered, its performance should improve. In the next section, we formalize these ideas and give a precise bound on the regret of posterior sampling. 
