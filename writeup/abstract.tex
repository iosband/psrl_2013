\begin{abstract}

Most provably-efficient learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for efficient exploration, \emph{posterior sampling for reinforcement learning} (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this \emph{sample} during the episode. The algorithm is conceptually simple, computationally efficient and allows an agent to encode prior knowledge in a natural way. We establish an $\tilde{O}(\tau S \sqrt{AT} )$ bound on the expected regret, where $T$ is time, $\tau$ is the episode length and $S$ and $A$ are the cardinalities of the state and action spaces. This bound is one of the first for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL significantly outperforms existing algorithms with similar regret bounds.


\end{abstract}


%We propose PSRL: a computationally efficient reinforcement learning algorithm based upon posterior sampling. The algorithm proceeds in episodes of known duration. At the beginning of each, a posterior distribution over Markov decision processes is computed, an instance is sampled from this distribution, and an optimal reward for this \emph{sample} is computed for use over this episode. We establish an $\tilde{O}(\tau S \sqrt{AT} )$ bound on the expected regret, where $T$ is time, $S$ and $A$ are the cardinalities of the state and action spaces, and $\tau$ is the episode length. In doing this, we develop some of the first regret bounds for a reinforcement learning algorithm not based upon optimism. We show that these bounds hold in expectation irrespective of prior, and with high probability unless the prior misspecification is exponentially large. This algorithm is computationally efficient, since for each episode it only requires solving for an optimal policy of a single MDP; it allows an agent to encode prior knowledge in a natural way, which is invaluable for practical applications; and PSRL outperforms existing algorithms with similar regret bounds by large margins in simulation.
